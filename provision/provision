# script for provisioning a cluster

hosts=(
	"127.0.0.1 adhoc-1"
	"127.0.0.1 adhoc-2"
	"127.0.0.1 namenode"
	"127.0.0.1 datanode1"
	"127.0.0.1 hiveserver"
	"127.0.0.1 hivemetastore"
	"127.0.0.1 kafkabroker"
	"127.0.0.1 sparkmaster"
	"127.0.0.1 zookeeper"
)



if [ -v "$(cat /etc/hosts | grep adhoc-1)" ]
then
	echo "adding hosts to /etc/hosts"
	sudo chown malanb5 /etc/hosts
	sudo chmod +w /etc/hosts
	for ((i=0; i < ${#hosts[@]}; i++))
	do
		printf "${hosts[$i]}\n" >> /etc/hosts
	done
	sudo chown root /etc/hosts
	sudo chmod -w /etc/hosts
else
	echo "hosts already found in file"
fi


hadoop_fp="/tmp/hadoop-download.src.tar.gz"
sudo apt-get update && sudo apt-get install maven kafkacat jq ssh pdsh openssh-server openssh-client

# download and move hadoop into /usr/local/hadoop
# update file permissions
# sudo chown -R hduser:hadoop /usr/local/hadoop
# sudo chmod -R 777 /usr/local/hadoop
#curl http://apache.mirrors.hoobly.com/hadoop/common/hadoop-3.2.1/hadoop-3.2.1-src.tar.gz --output $hadoop_fp

#tar -xf $hadoop_fp -C /tmp/hadoop
#/tmp/hadoop-download.src.tar.gz
# add the hadoop group and user
#sudo addgroup hadoop
#sudo adduser --ingroup hadoop hduser

# install ssh and open port 22
#sudo apt-get update
#sudo apt-get install openssh-server openssh-client
#sudo ufw allow 22
#sudo systemctl restart ssh
#sudo apt-get install ssh
#sudo apt-get install rsync

# disable ipv6
# Disable IPV6
#Hadoop & IPV6 does not agrees on the meaning of address 0.0.0.0 so we need to disable IPV6 editing the file…
#sudo nano /etc/sysctl.conf
#with…
#net.ipv6.conf.all.disable_ipv6=1
#net.ipv6.conf.default_ipv6=1
#net.ipv6.conf.lo.disable_ipv6=1
#
#
#
#
